---
title: AWS Notes
date: 2024-11-27 18:00:00 +1100
categories: [Cloud Computing, AWS]
tags:
  [
    aws cli,
    aws cdk,
    amazon bedrock,
    cloud development,
    docker,
    rag,
    genai,
    aws lambda,
    aws waf,
  ]
description: "Comprehensive notes on AWS CLI setup, AWS CDK configuration, and deploying Amazon Bedrock RAG using Docker. Includes troubleshooting tips and example commands."
---

## AWS Command Line Interface (CLI)

```shell
$ aws --version
aws-cli/2.18.6 Python/3.12.6 Darwin/24.0.0 exe/x86_64

$ which aws
/usr/local/bin/aws
```

### Setup

**Configuration**

- AWS access key ID
  - identity and access management (IAM) -> security credentials -> access keys -> create access key
- AWS secret access key
  - identity and access management (IAM) -> security credentials -> access keys -> create access key
- default region name
  - can be found on the aws console url
- default output format
  - left empty

```shell
$ aws configure
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: us-east-1
Default output format [None]:

# change region
$ aws configure set region <region-name>

# check configuration
$ aws configure list
      Name                    Value             Type    Location
      ----                    -----             ----    --------
   profile                <not set>             None    None
access_key     ****************H2GM shared-credentials-file
secret_key     ****************5h5u shared-credentials-file
    region                us-east-1      config-file    ~/.aws/config
```

## AWS Cloud Development Kit (CDK)

### setup

**Prerequisite**

- install Node.js

```shell
$ npm install -g aws-cdk
$ cdk --version
2.162.1 (build 10aa526)
```

**Configuration**

- CDK CLI will automatically use the security credentials that you configure with the AWS CLI

**Bootstrap**

```shell
$ cdk bootstrap aws://<aws-account-id>/<region>
```

## Amazon Bedrock

- [RAG sample repository](https://github.com/aws-samples/amazon-bedrock-rag/tree/main?tab=readme-ov-file)

### Setup

**Steps**

1. start Docker

   ```shell
   # check status
   $ docker -v
   Docker version 26.1.1, build 4cf5afa
   $ docker ps
   CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
   ```

2. deploy backend to AWS

   ```shell
   $ cd backend
   $ cdk deploy --context allowedip="xxx.xxx.xxx.xxx/32"
   ```

   - `allowedip` is your current IP address
     - google " what is my ip" to get public IP address

3. run frontend

   ```shell
   $ cd frontend
   $ npm install
   $ npm run start
   ```

> issues encountered
>
> - Rosetta and Docker compatibility issues
>   ```shell
>   # ensure Rosetta is installed
>   $ softwareupdate --install-rosetta --agree-to-license
>   # specify the correct platform in Docker
>   $ cdk deploy --context allowedip="xxx.xxx.xxx.xxx/32" --build-arg IMAGE=public.ecr.aws/sam/build-nodejs18.x --platform linux/arm64
>   ```
> - conflict of yarn versions, 1 for Hadoop, 1 for npm
>   - changed the Hadoop yarn path to resolve this issue
> - error when installing packages within Docker build
>
>   - comment out those line in `Dockerfile`, install manually
>   - `Dockerfile` located at backend -> node_modules -> aws-cdk-lib -> aws-lambda-nodejs -> lib -> Dockerfile
>
>   ```shell
>   $ npm install --global typescript
>   $ npm install --global pnpm@7.30.5
>   $ npm install --global --unsafe-perm=true esbuild@0.21
>   ```
{: .prompt-info }

**parameter settings**

- `KNOWLEDGE_BASE_ID`

  - can be store in AWS Systems Manager (SSM) Parameter Store

    ```shell
    $ aws ssm put-parameter --name "/bedrock/knowledge-base-id" --value "<KNOWLEDGE_BASE_ID>" --type "String"
    ```

  - retrieve the parameter value

    ```shell
    $ aws ssm get-parameter --name "/bedrock/knowledge-base-id" --with-decryption
    {
        "Parameter":
        {
            "Name": "/bedrock/knowledge-base-id",
            "Type": "String",
            "Value": "**********",
            "Version": 1,
            "LastModifiedDate": "2024-10-16T16:00:35.062000+11:00",
            "ARN": "arn:aws:ssm:us-east-1:************:parameter/bedrock/knowledge-base-id",
            "DataType": "text"
        }
    }
    ```

### Question and Answer

**Connection Error**

1. model response: _"Sorry, I am unable to assist you with this request."_

   - **issue**:
     - likely due to mismatch in knowledge base ID
   - **solution**:
     - update code to reference the existing knowledge base ID rather than generating a new one
   - **notes**:
     - check the log:
       - _CloudWatch -> Log groups -> /aws/lambda/query-bedrock-llm -> Configuration -> Environment variables -> KNOWLEDGE_BASE_ID_
       - or _Lambda -> Functions -> query-bedrock-llm -> Configuration -> Environment variables -> KNOWLEDGE_BASE_ID_
     - the original knowledge base and S3 bucket were created manually through the AWS console
     - code currently attempts to create new resources for Bedrock. Modify it to utilize the existing knowledge base ID and S3 bucket to avoid conflicts

2. model response: _"Error generating an answer. Please check your browser console, WAF configuration, Bedrock model access, and Lambda logs for debugging the error."_

   - **issue**:
     - the Web Application Firewall (WAF) is blocking requests, even if the same IP address was previously allowed
   - **solution**:
     - _WAF & Shield -> AWS WAF -> IP sets -> [IP sets name] -> add IP address_
   - **notes**:
     - the list of allowed IP addresses unexpectedly became empty, even though no manual deletions were made. Investigate potential automated processes or configurations that could cause this behavior

### Knowledge Base

**update data source**
1. update the files in S3 bucket
2. select data source in knowledge base and sync
   - sync failed: click into data souce, under sync history, select sync and click `view warnings` to look at detailed error message

#### Web Crawler

- allows integration of web crawling into bedrock for data ingestion
- supports structured and unstructured web content
- enables automatic extraction of text from HTML pages
- [reference](https://docs.aws.amazon.com/bedrock/latest/userguide/webcrawl-data-source-connector.html)

**setup**
- ensure **amazon bedrock** is enabled
- configure an **s3** bucket to store crawled data
- create a new data source connector for web crawling in bedrock

**prerequisite**
- ensure authorization to crawl source URLs
- verify that `robots.txt` allows crawling for source URLs
  - adheres to `robots.txt` standards as per [RFC 9309](https://www.rfc-editor.org/rfc/rfc9309.html)
  - defaults to `disallow` if `robots.txt` is not found
- check if source URLs are JavaScript dynamically generated
  - dynamically generated content is not supported
  - test by using `view-source:https://examplesite.com/site/` in a browser
  - disable JavaScript in the browser to see if content renders properly with links
- enable [CloudWatch Logs delivery](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-bases-logging.html)
  - monitor data ingestion job status
  - check for URLs that cannot be retrieved

**key configurations**
- start url
  - specify the starting point for crawling (e.g., `https://example.com`)
- recrawl interval
  - set how often the crawler revisits the website
- exclusion rules
  - define patterns to skip irrelevant URLs
- max crawl depth
  - limit the number of links followed

**permissions required**

- **s3** read/write access for storing crawled data
- specific **IAM** role to grant bedrock access to **s3** and web resources

**API**

```bash
aws bedrock create-data-source \
  --type "webcrawl" \
  --configuration "{
    \"startUrl\": \"https://example.com\",
    \"maxCrawlDepth\": 3,
    \"exclusionPatterns\": [\"*.jpg\", \"*.png\"]
  }" \
  --output "arn:aws:s3:::your-s3-bucket/webcrawl-output/"
```

```json
{
    "webConfiguration": {
        "sourceConfiguration": {
            "urlConfiguration": {
                "seedUrls": [{
                    "url": "https://www.examplesite.com"
                }]
            }
        },
        "crawlerConfiguration": {
            "crawlerLimits": {
                "rateLimit": 50
            },
            "scope": "HOST_ONLY",
            "inclusionFilters": [
                "https://www\.examplesite\.com/.*\.html"
            ],
            "exclusionFilters": [
                "https://www\.examplesite\.com/contact-us\.html"
            ]
        }
    },
    "type": "WEB"
}
```

**best practices**
- target relevant domains to reduce noise in crawled data
- use exclusion rules to skip large binary files (e.g., images, videos)
- monitor crawl logs in s3 to debug and optimize performance

**limitations**
- limited to public web content (no password-protected pages)
- requires manual setup of exclusion/inclusion rules
- heavily dependent on website structure for efficient crawling

#### custom data source

- **flexibility**: allows connection to data types beyond supported services
- **direct ingestion**: use `KnowledgeBaseDocuments` API to add or delete documents without syncing
- **document access**: view documents via the amazon bedrock console or API
- **upload options**: add documents directly through the AWS management console or inline
- **metadata management**: assign metadata to each document for filtering during retrieval
- [reference](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-data-source-connector.html)

**setup**

- send a `CreateDataSource` request to the **Agents for Amazon Bedrock** build-time endpoint
  - specify `knowledgeBaseId` of the target knowledge base
  - assign a `name` to the data source
  - set `type` in `dataSourceConfiguration` to `CUSTOM`

**optional configurations**

- `description`: provide details about the data source
- `clientToken`: ensure idempotency of the API request
- `serverSideEncryptionConfiguration`: specify a custom KMS key for transient data storage
- `dataDeletionPolicy`: decide to `RETAIN` or `DELETE` vector embeddings upon data source deletion
- `vectorIngestionConfiguration`: configure ingestion options, including:
  - `chunkingConfiguration`: define document chunking strategy
  - `parsingConfiguration`: set parsing strategy for the data source
  - `customTransformationConfiguration`: apply a lambda function for custom data transformation

**post-setup actions**

- add documents to the custom data source
- directly ingest documents into the knowledge base without the need for syncing

#### metadata filtering

- **enhanced retrieval**: improves search accuracy by filtering documents based on metadata attributes
- **custom metadata**: allows attachment of metadata files (up to 10 KB each) to documents in the knowledge base
- **filtering capabilities**: supports filtering by string, number, or boolean metadata
- **performance benefits**: reduces CPU usage and query costs by narrowing search scope
- [reference](https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-knowledge-bases-now-supports-metadata-filtering-to-improve-retrieval-accuracy/).

**use cases**

- **document chatbots**: refine searches by operating system or application version to avoid obsolete information
- **conversational search**: personalize results using filters like work groups or project IDs
- **developer search**: target specific release versions or document types for precise information retrieval

**implementation steps**

1. **prepare metadata files**: create `.metadata.json` files with the same name as the source data files
   - example content:

     ```json
     {
       "metadataAttributes": {
         "tag": "project EVE",
         "year": 2016,
         "team": "ninjas"
       }
     }
     ```

2. **ingest data**: upload source and metadata files to an Amazon S3 bucket
3. **create knowledge base**: set up a new knowledge base in Amazon Bedrock
4. **synchronize data**: sync the S3 bucket with the knowledge base to ingest documents and metadata

**querying with metadata filters**

- **console**:
  - navigate to the knowledge base
  - select "Test knowledge base"
  - apply filters in the format `key = value` (e.g., `genres = Strategy`)
  - enter query and run

- **sdk**:
  - construct filter expressions using operators like `equals`, `greaterThanOrEquals`, etc.
  - pass filters to the `retrievalConfiguration` in the `Retrieve` or `RetrieveAndGenerate` API calls

**availability**

- currently available in **US East (N. Virginia)** and **US West (Oregon)** AWS regions

## AWS Amplify

### Deploy frontend

1. build the app
    ```shell
    $ npm run build
    ```
2. zip only the content of the build folder, not zipping the folder itself
3. create a new app -> host web app
4. upload build folder
5. save and deploy

### Custom Domain with different URL

1. app -> hosting
2. select custom domains
3. click **add domain button**
4. select available domain from drop down list
5. add redirection if required

## AWS Lambda

### add new API to API Gateways

```js
const chatsResource = apiGateway.root.addResource("chats");

chatsResource.addMethod("GET", new apigw.LambdaIntegration(lambdaQuery));
chatsResource.addMethod("POST", new apigw.LambdaIntegration(lambdaQuery));

const chatIdResource = chatsResource.addResource("{chatId}");
chatIdResource.addMethod(
  "DELETE",
  new apigw.LambdaIntegration(lambdaQuery)
);
```

### add new model

**grant IAM policies access**
1. navigate to IAM -> Access manamegent -> Policies -> AmazonBedrockFoundationModelPolicyForKnowledgeBase_<id>
2. click Edit button on top right, add model in resource, example below

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "BedrockInvokeModelStatement",
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel"
            ],
            "Resource": [
                "arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3",
                "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0",
                "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0"
            ]
        }
    ]
}
```